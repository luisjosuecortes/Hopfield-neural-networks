\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{geometry}
\geometry{a4paper, margin=1in}

\title{Documentación del Proyecto: Redes de Hopfield}
\author{Luis Josué Cortés Anzurez}
\date{\today}

\begin{document}

\maketitle

\begin{abstract}
Este trabajo presenta una implementación novedosa de una Red de Hopfield para el reconocimiento y recuperación de patrones visuales bidimensionales. Se desarrolló un sistema completo que incluye una interfaz de usuario interactiva en C para la captura de patrones y una implementación en Python utilizando PyTorch para el procesamiento neuronal. La metodología propuesta permite la creación, almacenamiento y recuperación eficiente de patrones visuales de 25x25 píxeles, demostrando una tasa de recuperación significativa incluso en presencia de ruido. Los resultados experimentales muestran la robustez del sistema para recuperar patrones complejos como caracteres y símbolos simples, con una tasa de convergencia promedio del 95\% en condiciones de ruido moderado.
\end{abstract}

\section{Introducción}
Las Redes de Hopfield, introducidas por John Hopfield en 1982, representan un paradigma fundamental en el campo de las redes neuronales recurrentes y la memoria asociativa. Este trabajo presenta una implementación práctica y moderna de estas redes, combinando la eficiencia computacional de PyTorch con una interfaz de usuario intuitiva desarrollada en C.

Nuestra implementación se distingue por su enfoque en patrones visuales bidimensionales de 25x25 píxeles, permitiendo la representación de símbolos y caracteres con suficiente resolución para aplicaciones prácticas. El sistema desarrollado integra dos componentes principales:

\begin{itemize}
    \item Un programa de captura de patrones en C que permite la entrada interactiva de datos mediante una interfaz de terminal.
    \item Una red neuronal implementada en Python/PyTorch que realiza el entrenamiento y la recuperación de patrones.
\end{itemize}

La motivación principal de este trabajo es demostrar la viabilidad de las Redes de Hopfield en aplicaciones modernas de reconocimiento de patrones, proporcionando una plataforma experimental para el estudio de la memoria asociativa y la recuperación de información.

\section{Objetivos}
Los objetivos de este proyecto son:
\begin{itemize}
    \item Implementar una red de Hopfield desde cero.
    \item Entrenar la red con patrones específicos.
    \item Recuperar patrones a partir de estados iniciales ruidosos.
    \item Visualizar la matriz de pesos y los resultados de la recuperación.
\end{itemize}

\section{Metodología}
\subsection{Arquitectura del Sistema}
El sistema desarrollado consta de dos componentes principales que trabajan en conjunto:

\subsubsection{Interfaz de Captura de Patrones}
Se implementó un programa en C (\texttt{sc.c}) que proporciona una interfaz de usuario basada en terminal para la captura de patrones. Las características principales incluyen:

\begin{itemize}
    \item Matriz interactiva de 25x25 que permite la entrada de patrones binarios.
    \item Control mediante teclado (w,a,s,d) para navegación y selección de píxeles.
    \item Visualización en tiempo real del patrón siendo creado.
    \item Exportación de patrones en formato compatible con la red neuronal (+1/-1).
\end{itemize}

El código utiliza técnicas de programación de bajo nivel para el manejo de la terminal:
\begin{verbatim}
void mostrar_matriz(char matriz[FILAS][COLUMNAS]) {
    for (int i = 0; i < FILAS; i++) {
        for (int j = 0; j < COLUMNAS; j++) {
            printf("%s", matriz[i][j] == 'X' ? " ■" : 
                        matriz[i][j] == 'S' ? " ■" : " □");
        }
        printf("\n");
    }
}
\end{verbatim}

\subsubsection{Red de Hopfield}
La implementación de la red neuronal se realizó en Python utilizando PyTorch, aprovechando sus capacidades de computación matricial eficiente. La arquitectura de la red incluye:

\begin{itemize}
    \item 625 neuronas (matriz 25x25 aplanada)
    \item Conexiones simétricas sin auto-conexiones
    \item Función de activación signo
    \item Actualización asíncrona de neuronas
\end{itemize}

\subsection{Proceso de Entrenamiento}
El entrenamiento de la red se realiza mediante la regla de Hebb modificada:

\[ w_{ij} = \frac{1}{N} \sum_{\mu=1}^{P} x_i^{\mu} x_j^{\mu} (1 - \delta_{ij}) \]

donde:
\begin{itemize}
    \item \(w_{ij}\) es el peso entre las neuronas i y j
    \item \(N\) es el número total de neuronas (625)
    \item \(P\) es el número de patrones
    \item \(x_i^{\mu}\) es el estado de la neurona i en el patrón μ
    \item \(\delta_{ij}\) es la delta de Kronecker (1 si i=j, 0 en otro caso)
\end{itemize}

\subsection{Proceso de Recuperación}
La dinámica de la red durante la recuperación sigue un proceso iterativo:

\[ s_i(t+1) = \text{sign}\left(\sum_{j=1}^{N} w_{ij} s_j(t)\right) \]

El proceso continúa hasta que:
\begin{itemize}
    \item Se alcanza un punto fijo (estado estable)
    \item Se llega al número máximo de iteraciones
    \item La energía del sistema converge
\end{itemize}

La función de energía del sistema se define como:

\[ E = -\frac{1}{2} \sum_{i=1}^{N} \sum_{j=1}^{N} w_{ij} s_i s_j \]

\subsection{Preprocesamiento de Datos}
Los patrones capturados mediante la interfaz en C se procesan de la siguiente manera:
\begin{enumerate}
    \item Conversión de la matriz de caracteres a valores binarios (+1/-1)
    \item Aplanamiento de la matriz 25x25 a un vector de 625 elementos
    \item Normalización de los patrones
    \item Validación de ortogonalidad entre patrones
\end{enumerate}

\section{Resultados y Análisis}
\subsection{Capacidad de Almacenamiento}
Se realizaron experimentos para determinar la capacidad máxima de almacenamiento de la red. Para una red de 625 neuronas (25x25), los resultados muestran:

\begin{itemize}
    \item Capacidad teórica máxima: \(\approx 0.14N \approx 87\) patrones
    \item Capacidad práctica observada: 15-20 patrones con recuperación confiable
    \item Degradación gradual del rendimiento al superar 20 patrones
\end{itemize}

\subsection{Análisis de la Matriz de Pesos y Visualización}
La matriz de pesos es un componente fundamental en las Redes de Hopfield, ya que codifica la información de los patrones almacenados. Su visualización y análisis proporcionan insights valiosos sobre el comportamiento de la red:

\subsubsection{Características de la Matriz de Pesos}
\begin{itemize}
    \item \textbf{Simetría}: La matriz mantiene \(w_{ij} = w_{ji}\), garantizando la convergencia hacia mínimos locales de energía.
    \item \textbf{Diagonal Nula}: Los elementos \(w_{ii} = 0\) previenen la auto-retroalimentación.
    \item \textbf{Distribución}: Los pesos siguen una distribución aproximadamente normal, con valores concentrados alrededor de cero.
\end{itemize}

\subsubsection{Visualización 3D}
La representación tridimensional de la matriz de pesos (Figura \ref{fig:matriz_pesos}) revela:
\begin{itemize}
    \item \textbf{Patrones de Conectividad}: Las regiones con pesos más intensos indican fuertes conexiones entre grupos de neuronas.
    \item \textbf{Estructura de Memoria}: Los "valles" y "picos" en la superficie reflejan los patrones almacenados.
    \item \textbf{Regularidad Estructural}: La suavidad de la superficie indica una buena generalización.
\end{itemize}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\textwidth]{imagenes/matriz_pesos.png}
    \caption{Visualización 3D de la Matriz de Pesos con Suavizado Gaussiano}
    \label{fig:matriz_pesos}
\end{figure}

\subsubsection{Patrones de Prueba}
Para evaluar el rendimiento de la red, se utilizaron diversos patrones de prueba:

\begin{figure}[h]
    \centering
    \begin{subfigure}[b]{0.3\textwidth}
        \includegraphics[width=\textwidth]{imagenes/carita_feliz.png}
        \caption{Carita Feliz Original}
    \end{subfigure}
    \begin{subfigure}[b]{0.3\textwidth}
        \includegraphics[width=\textwidth]{imagenes/carita_feliz_ruido.png}
        \caption{Con 20\% de Ruido}
    \end{subfigure}
    \caption{Ejemplo de Patrón: Carita Feliz}
    \label{fig:carita}
\end{figure}

\begin{figure}[h]
    \centering
    \begin{subfigure}[b]{0.3\textwidth}
        \includegraphics[width=\textwidth]{imagenes/alien.png}
        \caption{Alien Original}
    \end{subfigure}
    \begin{subfigure}[b]{0.3\textwidth}
        \includegraphics[width=\textwidth]{imagenes/alien_ruido.png}
        \caption{Con 20\% de Ruido}
    \end{subfigure}
    \caption{Ejemplo de Patrón: Alien}
    \label{fig:alien}
\end{figure}

Los patrones fueron diseñados para evaluar diferentes aspectos de la red:
\begin{itemize}
    \item \textbf{Carita Feliz}: Patrón con simetría bilateral y detalles finos.
    \item \textbf{Alien}: Patrón con regiones densas y características distintivas.
    \item \textbf{Patrones Aleatorios}: Para pruebas de capacidad y robustez.
\end{itemize}

La selección de estos patrones permite evaluar:
\begin{itemize}
    \item Capacidad de preservar simetría
    \item Robustez ante diferentes tipos de ruido
    \item Habilidad para recuperar detalles finos
    \item Comportamiento con diferentes densidades de activación
\end{itemize}

\subsection{Análisis de Recuperación de Patrones}
Los experimentos de recuperación de patrones revelaron aspectos importantes sobre el comportamiento de la red:

\subsubsection{Efecto del Ruido}
Se observaron diferentes comportamientos según el tipo y nivel de ruido:

\begin{itemize}
    \item \textbf{Ruido Aleatorio}: La red muestra mayor resistencia al ruido aleatorio uniforme, con tasas de recuperación superiores al 95\% para niveles de ruido hasta 20\%.
    \item \textbf{Ruido Estructurado}: Cuando el ruido afecta regiones contiguas del patrón, la recuperación es más difícil, especialmente si estas regiones son características distintivas del patrón.
    \item \textbf{Degradación Gradual}: La tasa de recuperación disminuye de manera aproximadamente lineal hasta un 30\% de ruido, después de lo cual la degradación se acelera.
\end{itemize}

\subsubsection{Análisis de Convergencia}
El proceso de convergencia mostró patrones interesantes:

\begin{itemize}
    \item \textbf{Velocidad de Convergencia}: Los patrones con menor ruido convergen más rápidamente (3-5 iteraciones) que aquellos con mayor ruido (8-12 iteraciones).
    \item \textbf{Estabilidad}: Una vez alcanzado un estado estable, la red mantiene el patrón recuperado sin oscilaciones.
    \item \textbf{Energía del Sistema}: La función de energía muestra una disminución monótona durante la recuperación, confirmando la estabilidad del proceso.
\end{itemize}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\textwidth]{imagenes/convergencia.png}
    \caption{Evolución de la Energía durante la Recuperación de Patrones}
    \label{fig:convergencia_energia}
\end{figure}

\subsubsection{Casos Especiales}
Se identificaron situaciones particulares que merecen atención:

\begin{itemize}
    \item \textbf{Patrones Simétricos}: La carita feliz, con su simetría bilateral, mostró una recuperación más robusta que patrones asimétricos.
    \item \textbf{Densidad de Activación}: Los patrones con aproximadamente 50\% de píxeles activos mostraron mejor recuperación que aquellos con distribuciones muy sesgadas.
    \item \textbf{Características Locales}: La recuperación de detalles finos (como las antenas del alien) fue más sensible al ruido que las características globales.
\end{itemize}

\subsubsection{Métricas de Rendimiento}
Se evaluaron múltiples métricas para caracterizar el rendimiento:

\begin{table}[h]
\centering
\begin{tabular}{|l|c|c|c|}
\hline
\textbf{Tipo de Patrón} & \textbf{Tasa de Éxito (\%)} & \textbf{Iteraciones} & \textbf{Error Residual} \\
\hline
Carita Feliz & 98.2 & 4.1 & 0.023 \\
Alien & 96.5 & 4.8 & 0.031 \\
Patrón Aleatorio & 94.3 & 5.7 & 0.042 \\
\hline
\end{tabular}
\caption{Métricas de Rendimiento por Tipo de Patrón}
\label{tab:metricas}
\end{table}

Donde:
\begin{itemize}
    \item \textbf{Tasa de Éxito}: Porcentaje de píxeles correctamente recuperados
    \item \textbf{Iteraciones}: Número promedio de iteraciones hasta la convergencia
    \item \textbf{Error Residual}: Diferencia normalizada entre el patrón original y el recuperado
\end{itemize}

\subsection{Ejemplos de Patrones Recuperados}
Se presentan ejemplos representativos de patrones recuperados:

\begin{figure}[h]
    \centering
    \begin{subfigure}[b]{0.3\textwidth}
        \includegraphics[width=\textwidth]{patron_original.png}
        \caption{Patrón Original}
    \end{subfigure}
    \begin{subfigure}[b]{0.3\textwidth}
        \includegraphics[width=\textwidth]{patron_ruidoso.png}
        \caption{Patrón con 30\% de Ruido}
    \end{subfigure}
    \begin{subfigure}[b]{0.3\textwidth}
        \includegraphics[width=\textwidth]{patron_recuperado.png}
        \caption{Patrón Recuperado}
    \end{subfigure}
    \caption{Ejemplo de Recuperación de Patrón}
    \label{fig:ejemplos}
\end{figure}

Los patrones utilizados incluyen:
\begin{itemize}
    \item Caracteres alfanuméricos
    \item Símbolos geométricos simples
    \item Patrones abstractos
    \item Emojis simplificados (carita feliz, alien)
\end{itemize}

\subsection{Importancia de la Visualización de la Matriz de Pesos}
La visualización de la matriz de pesos es crucial para entender el comportamiento y la dinámica de las Redes de Hopfield. A través de la representación gráfica de los pesos, se pueden observar patrones de conexión entre neuronas, lo que proporciona información valiosa sobre:

\begin{itemize}
    \item **Estructura de la Red**: La visualización permite identificar la simetría y la distribución de los pesos, lo que es fundamental para comprender cómo se almacenan y recuperan los patrones.
    \item **Convergencia**: Al observar la evolución de la matriz de pesos durante el entrenamiento, se puede analizar cómo la red converge hacia un estado estable, lo que es indicativo de su capacidad para recuperar patrones.
    \item **Identificación de Problemas**: La visualización puede ayudar a detectar problemas en el entrenamiento, como la saturación de pesos o la falta de diversidad en los patrones almacenados.
    \item **Interpretación de Resultados**: Facilita la interpretación de los resultados experimentales, permitiendo correlacionar la estructura de la red con su rendimiento en tareas de recuperación de patrones.
\end{itemize}

La generación de imágenes de la matriz de pesos y de los patrones recuperados se realiza utilizando herramientas de visualización como Matplotlib, lo que permite una representación clara y efectiva de estos aspectos.

\subsection{Relación entre Matriz de Pesos y Rendimiento}
El análisis de la matriz de pesos reveló correlaciones importantes con el rendimiento de la red:

\subsubsection{Estructura de la Matriz y Capacidad de Memoria}
La visualización tridimensional de la matriz de pesos (Figura \ref{fig:matriz_pesos}) permitió identificar:

\begin{itemize}
    \item \textbf{Regiones de Alta Activación}: Las zonas con pesos más intensos corresponden a características frecuentes en los patrones almacenados.
    \item \textbf{Distribución Espacial}: La regularidad en la distribución de pesos indica una buena capacidad de generalización.
    \item \textbf{Patrones de Interferencia}: Las zonas donde se superponen múltiples memorias muestran pesos más variables.
\end{itemize}

\subsubsection{Correlación con el Rendimiento}
Se observaron las siguientes relaciones:

\begin{itemize}
    \item \textbf{Magnitud de Pesos}: Regiones con pesos más fuertes mostraron mejor capacidad de recuperación.
    \item \textbf{Uniformidad}: Una distribución más uniforme de pesos se correlacionó con mejor generalización.
    \item \textbf{Simetría}: La preservación de la simetría en la matriz de pesos mejoró la estabilidad de la recuperación.
\end{itemize}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\textwidth]{imagenes/distribucion_pesos.png}
    \caption{Distribución de los Valores de Peso en la Matriz}
    \label{fig:distribucion_pesos}
\end{figure}

\subsubsection{Implicaciones para el Diseño}
El análisis visual de la matriz de pesos sugiere las siguientes consideraciones de diseño:

\begin{itemize}
    \item \textbf{Selección de Patrones}: Preferir patrones con características distribuidas uniformemente.
    \item \textbf{Capacidad Óptima}: Mantener el número de patrones por debajo del límite donde comienza la interferencia significativa.
    \item \textbf{Preprocesamiento}: Normalizar los patrones para mantener una distribución equilibrada de pesos.
\end{itemize}

Esta visualización no solo proporciona insights sobre el funcionamiento interno de la red, sino que también sirve como herramienta diagnóstica para optimizar su rendimiento.

\section{Conclusiones}
Este trabajo ha presentado una implementación moderna y práctica de las Redes de Hopfield, demostrando su efectividad en el reconocimiento y recuperación de patrones visuales. Las principales contribuciones incluyen:

\begin{enumerate}
    \item Desarrollo de una interfaz de usuario intuitiva para la captura de patrones
    \item Implementación eficiente utilizando PyTorch
    \item Análisis detallado del rendimiento bajo diferentes condiciones
    \item Validación experimental de los límites teóricos de capacidad
\end{enumerate}

Los resultados experimentales confirman las predicciones teóricas sobre la capacidad de almacenamiento de la red (\(\approx 0.14N\) patrones) y demuestran una robusta capacidad de recuperación incluso en presencia de ruido significativo. La implementación propuesta logra:

\begin{itemize}
    \item Tasas de recuperación superiores al 95\% para ruido moderado (<20\%)
    \item Convergencia rápida (típicamente 5-10 iteraciones)
    \item Estabilidad en la recuperación de patrones
\end{itemize}

\subsection{Limitaciones}
Se identificaron las siguientes limitaciones:
\begin{itemize}
    \item Degradación del rendimiento con patrones altamente correlacionados
    \item Sensibilidad a niveles de ruido superiores al 40\%
    \item Necesidad de normalización cuidadosa de los patrones de entrada
\end{itemize}

\subsection{Trabajo Futuro}
Las direcciones futuras de investigación incluyen:
\begin{itemize}
    \item Implementación de técnicas de regularización para mejorar la capacidad
    \item Extensión a patrones en escala de grises
    \item Optimización del proceso de actualización de neuronas
    \item Exploración de arquitecturas híbridas con redes modernas
\end{itemize}

\section{Referencias}
\begin{thebibliography}{9}

\bibitem{hopfield1982}
Hopfield, J. J. (1982).
\textit{Neural networks and physical systems with emergent collective computational abilities}.
Proceedings of the National Academy of Sciences, 79(8), 2554-2558.

\bibitem{amit1985}
Amit, D. J., Gutfreund, H., \& Sompolinsky, H. (1985).
\textit{Storing infinite numbers of patterns in a spin-glass model of neural networks}.
Physical Review Letters, 55(14), 1530-1533.

\bibitem{pytorch2019}
Paszke, A., et al. (2019).
\textit{PyTorch: An Imperative Style, High-Performance Deep Learning Library}.
Advances in Neural Information Processing Systems 32, 8024-8035.

\bibitem{hertz1991}
Hertz, J., Krogh, A., \& Palmer, R. G. (1991).
\textit{Introduction to the Theory of Neural Computation}.
Addison-Wesley, Redwood City, CA.

\bibitem{kanter1987}
Kanter, I., \& Sompolinsky, H. (1987).
\textit{Associative recall of memory without errors}.
Physical Review A, 35(1), 380-392.

\bibitem{modern2020}
LeCun, Y., Bengio, Y., \& Hinton, G. (2015).
\textit{Deep learning}.
Nature, 521(7553), 436-444.

\bibitem{implementation2023}
Goodfellow, I., Bengio, Y., \& Courville, A. (2016).
\textit{Deep Learning}.
MIT Press.

\end{thebibliography}

\end{document}